{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f823935b-9f86-4ec5-918c-14132163ab72",
   "metadata": {},
   "source": [
    "# Projets Retrieval-Augmented Generation (RAG)\n",
    "- Chatbot pour FAQ pour une entreprise de gestion des fonds immobiliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eba25a",
   "metadata": {},
   "source": [
    "## Installation des modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c67b9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un environnement virtuel nommé myenv et installer les librairies\n",
    "! python -m venv myenv\n",
    "! .\\myenv\\Scripts\\activate\n",
    "! python.exe -m pip install --upgrade pip\n",
    "! pip install requirements.txt\n",
    "! pip list "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf6d419-4997-4239-b84b-c4b857640aae",
   "metadata": {},
   "source": [
    "## Chargement du LLM et définition de la clé OPENAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "599d1682-dac0-46f9-92f8-9015aa9936d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_12008\\1995061228.py:8: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Définir la clé API OpenAI\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "# Initialiser le modèle GPT-4 \n",
    "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848d5ad8-b2e2-45fe-84f2-9d282dfc4114",
   "metadata": {},
   "source": [
    "### 1. Chargement des documents d'information sur l'entreprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "379ea608-6e7e-4932-bf7c-d275678d80d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "Document_information_clés = PyPDFLoader(\"Docs/DIC-IR-062024-FR-1-1542.pdf\")\n",
    "Fiche_produit = PyPDFLoader(\"Docs/FP-IR-032025-FR-2-2056.pdf\")\n",
    "pages_DIC = Document_information_clés.load()\n",
    "pages_FP = Fiche_produit.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fab9491-579d-44f9-8305-e6540a1a342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIC = pages_DIC[0]\n",
    "print(DIC.page_content[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e2422e-711c-445b-8b85-63dcd29fa1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIC.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87055e4-6694-44fe-a753-1c6e9a9263dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "FP = pages_FP[0]\n",
    "print(FP.page_content[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1304d9b1-ce2b-4af0-85cf-219294ea0508",
   "metadata": {},
   "outputs": [],
   "source": [
    "FP.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7f183d-2c3c-48d1-a448-513166cf3b12",
   "metadata": {},
   "source": [
    "### 2. Fractionnement des documents (Document Splitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41058f9-bf9e-49d4-81c8-93ff098a2341",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "# Extraction du texte du document DIC\n",
    "text_DIC = DIC.page_content   \n",
    "\n",
    "\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=450,\n",
    "    chunk_overlap=0, \n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Appliquer le fractionnement\n",
    "r_splitter.split_text(text_DIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ba9619-e408-457b-adc5-01da0d444154",
   "metadata": {},
   "source": [
    "### 3. Vectorstores and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9956d4a9-0fa9-4703-ace8-ffce36517f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings pour convertir le texte des FAQ en vecteurs, ce qui permettra d'effectuer des recherches basées sur la similarité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2781a2d7-a564-4cd5-85c2-43b77d2c33d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fd67a5-2434-430c-8bbb-bcf23abd4fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install tf-keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cda20c-8b32-4428-b6f7-73c24122f0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Créer un modèle d'embedding avec HuggingFace\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Embedding des documents\n",
    "doc_texts = [doc.page_content for doc in chunked_documents]\n",
    "embeddings = [embedding_model.embed_query(text) for text in doc_texts]\n",
    "\n",
    "# Initialisation de l'index FAISS\n",
    "dimension = len(embeddings[0])  # Dimension des embeddings\n",
    "index = faiss.IndexFlatL2(dimension)  # Utilisation de la distance Euclidienne\n",
    "index.add(np.array(embeddings))\n",
    "\n",
    "# Créer un vector store FAISS\n",
    "docstore = FAISS(embedding_function=embedding_model, index=index, docstore=dict(enumerate(chunked_documents)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93de1dc-e95b-476a-9302-a164e7cb8b56",
   "metadata": {},
   "source": [
    "### 4. Créer le modèle RAG (Retrieval-Augmented Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3804ed0-8155-44ef-9ae3-9ae952a5cfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intégrer un modèle de génération de texte comme GPT-2 qui générera des réponses basées sur les documents récupérés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091f1c7b-b61a-46df-90ce-ccee3d0a0e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Charger le modèle GPT-4 via OpenAI API\n",
    "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)  # GPT-4 pour des réponses précises\n",
    "\n",
    "# Créer un 'retriever' pour récupérer les documents les plus pertinents\n",
    "retriever = docstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
    "\n",
    "# Créer la chaîne RetrievalQA avec le modèle GPT-4 et le retriever\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # Tous les documents récupérés sont envoyés au modèle\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36db3639-2bb2-4e92-b30b-d30437fe72da",
   "metadata": {},
   "source": [
    "### 5. Ajouter un système de feedback utilisateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecbca42-e181-4f33-a59d-8bc5fee212ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour améliorer le modèle au fil du temps, un système de feedback permet aux utilisateurs de valider ou d'améliorer les réponses générées. \n",
    "# Cela pourrait impliquer de stocker des réponses et de les annoter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b6e770-e7e8-4b3e-9163-48662606f217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un dictionnaire pour stocker les retours utilisateurs\n",
    "feedback_store = {}\n",
    "\n",
    "def collect_feedback(question, response, feedback):\n",
    "    feedback_store[question] = {\"response\": response, \"feedback\": feedback}\n",
    "    print(\"Feedback enregistré :\", feedback_store)\n",
    "\n",
    "# Exemple de collecte de feedback\n",
    "collect_feedback(\"Quels sont les frais de souscription ?\", \"10% HT\", \"Correct\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8b72ff-6cf7-4308-8096-b2853c350348",
   "metadata": {},
   "source": [
    " ### 6. Streamlit (Chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ceb866-e641-4491-a7eb-8e94e93f45b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamlit sera utilisé pour créer une interface où les utilisateurs peuvent poser des questions et obtenir des réponses en temps réel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16996b16-94a9-4da0-9ccf-908ab89de3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "\n",
    "# Fonction pour interroger le modèle et afficher les réponses\n",
    "def display_chat():\n",
    "    st.title(\"Chatbot FAQ - Gestion des Fonds Immobiliers\")\n",
    "    \n",
    "    # Entrée utilisateur\n",
    "    user_question = st.text_input(\"Posez votre question :\")\n",
    "    \n",
    "    if user_question:\n",
    "        # Obtenir la réponse à partir du modèle RAG\n",
    "        response = qa_chain({\"query\": user_question})\n",
    "        \n",
    "        # Afficher la réponse\n",
    "        st.write(\"Réponse :\", response['result'])\n",
    "        \n",
    "        # Collecter le feedback utilisateur\n",
    "        feedback = st.radio(\"Feedback\", [\"Correct\", \"Incorrect\"])\n",
    "        \n",
    "        # Enregistrer le feedback\n",
    "        if st.button(\"Envoyer le feedback\"):\n",
    "            collect_feedback(user_question, response['result'], feedback)\n",
    "\n",
    "# Lancer l'interface Streamlit\n",
    "if __name__ == '__main__':\n",
    "    display_chat()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614afa80-a0da-45ef-a788-5083f4aaadc9",
   "metadata": {},
   "source": [
    "### 7. Amélioration avec l'ajout dynamique de FAQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9315b5ba-7368-4fd3-8d68-661e73096218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un autre module permettra l'ajout de nouvelles FAQ à la base de données. \n",
    "# Cela pourrait être intégré dans une interface d'administration où de nouvelles questions et réponses peuvent être ajoutées au fil du temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028155b5-ed1d-4cec-9d0c-6963bc53205f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_faq(new_question, new_answer):\n",
    "    new_doc = Document(page_content=new_answer)\n",
    "    documents.append(new_doc)\n",
    "    \n",
    "    # Réindexer les nouveaux documents\n",
    "    chunked_documents = text_splitter.split_documents(documents)\n",
    "    doc_texts = [doc.page_content for doc in chunked_documents]\n",
    "    embeddings = [embedding_model.embed_query(text) for text in doc_texts]\n",
    "    index.add(np.array(embeddings))\n",
    "    \n",
    "    # Réinitialiser le vector store\n",
    "    docstore = FAISS(embedding_function=embedding_model, index=index, docstore=dict(enumerate(chunked_documents)))\n",
    "    print(\"Nouvelle FAQ ajoutée et base mise à jour.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6782e7b9-f707-4cab-8f93-bcd3d1ca302e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
